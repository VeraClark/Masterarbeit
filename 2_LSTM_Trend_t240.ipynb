{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Packages laden\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score,classification_report,confusion_matrix,auc, log_loss \n",
    "from pandas import DataFrame\n",
    "from keras.backend import binary_crossentropy\n",
    "import tensorflow as tf\n",
    "from pandas import concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replizierbarkeit gewährleisten\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden der Daten\n",
    "DATA_FILE=\"Dropbox/Masterarbeit/Masterarbeit/Datensätze/Geburtenrate/Datensatz.xlsx\"\n",
    "df = pd.read_excel(DATA_FILE)\n",
    "df=pd.read_excel(DATA_FILE, header=0, parse_dates=[0], index_col='Jahr-Monat',squeeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ein array mit allen Einträgen --> zur besseren Weiterverarbeitung\n",
    "ZR=df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skalierung [-1,1]\n",
    "def scaling(dataset,l_bound=-1, h_bound=1):\n",
    "    dataset = dataset.reshape(len(dataset), 1)\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(dataset)\n",
    "    scaled_dataset = scaler.transform(dataset)\n",
    "    return scaled_dataset\n",
    "\n",
    "# Umkehrung der Skalierung\n",
    "def invert_scale(scaler, X, value):\n",
    "    new_row = [x for x in X] + [value]\n",
    "    array = np.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "ZR_N=scaling(ZR)\n",
    "\n",
    "# Anteil der Datensätze am Gesamtdatensatz\n",
    "ZR_100=ZR\n",
    "\n",
    "# Unterteilung der Teildatensätze in Trainings-und Testdaten\n",
    "ZR_100_TR, ZR_100_TE= ZR_100[0:605], ZR_100[605:len(ZR_100)]\n",
    "\n",
    "# Anteil der Datensätze am Gesamtdatensatz: skalierte Daten\n",
    "ZR_N100=ZR_N\n",
    "\n",
    "# Unterteilung der Teildatensätze in Trainings-und Testdaten\n",
    "ZR_N100_TR, ZR_N100_TE= ZR_N100[0:605], ZR_N100[605:len(ZR_N100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Erzeugung von y zur Trendvorhersage/Klassifikation t+1: y=(-1,0,1)\n",
    "def create_Y_trend(dataset):\n",
    "    dataY=[0]*(len(dataset)-1)\n",
    "    for i in range(len(dataset)-1):\n",
    "        a = dataset[i]-dataset[i+1]\n",
    "        if a<0:\n",
    "            dataY[i]=1\n",
    "        elif a>0:\n",
    "            dataY[i]=-1\n",
    "        else:\n",
    "            dataY[i]=0\n",
    "    return np.array(dataY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Weiterentwicklung von Trendergebnissen y(t+1) zu y(t+look_forward) mit look_forward>1\n",
    "def create_Y_multitrend(dataset,look_forward):\n",
    "    dataY= create_Y_trend(dataset)\n",
    "    length=len(dataY)-look_forward+1\n",
    "    dataY_m=[0]*length \n",
    "    dataY_m=np.array(dataY_m)\n",
    "    for i in range(len(dataY_m)):\n",
    "        for j in range(look_forward):\n",
    "            dataY_m[i]= dataY_m[i]+dataY[i+j]\n",
    "        if dataY_m[i]<0:\n",
    "            dataY_m[i]=-1\n",
    "        elif dataY_m[i]>0:\n",
    "            dataY_m[i]=1\n",
    "        else:\n",
    "            dataY_m[i]=0\n",
    "    return np.array(dataY_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Erzeugung von x (Input) mit look_back (Anzahl der verwendeten Vergangenheitswerte)\n",
    "def X_to_supervised_T(dataset, look_back=1, look_forward=1):\n",
    "    df = DataFrame(dataset)\n",
    "    columns = [df.shift(look_forward-1+i) for i in range(1, look_back+1)]\n",
    "    df = concat(columns, axis=1)\n",
    "    df = df.drop(0)\n",
    "    DF=df[::-1]\n",
    "    df=DF[:len(df)-(look_back+look_forward-2)]\n",
    "    DF=df[::-1]\n",
    "    return DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Festlegung: look_back, look_forward + Anwendung der Funktion\n",
    "# Inputneuronen: 12\n",
    "# Vorhersagehorizont: t+240\n",
    "look_back=12\n",
    "look_forward=240\n",
    "\n",
    "# Erzeugen von y \n",
    "\n",
    "# Dateninput 100%\n",
    "# Skalierte Daten\n",
    "trainY_N100=create_Y_multitrend(ZR_N100_TR,look_forward)\n",
    "testY_N100=create_Y_multitrend(ZR_N100_TE,look_forward)\n",
    "\n",
    "# Original Daten\n",
    "trainY_100=create_Y_multitrend(ZR_100_TR,look_forward)\n",
    "testY_100=create_Y_multitrend(ZR_100_TE,look_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Erzeugen von X \n",
    "\n",
    "# Dateninput: 100%\n",
    "# Skalierte Daten\n",
    "trainX_N100=X_to_supervised_T(ZR_N100_TR,look_back,look_forward)\n",
    "testX_N100=X_to_supervised_T(ZR_N100_TE,look_back,look_forward)\n",
    "\n",
    "#Originaldaten\n",
    "trainX_100=X_to_supervised_T(ZR_100_TR,look_back,look_forward)\n",
    "testX_100=X_to_supervised_T(ZR_100_TE,look_back,look_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(354, 12) (354,)\n",
      "(7, 12) (7,)\n",
      "(354, 12) (354,)\n",
      "(7, 12) (7,)\n"
     ]
    }
   ],
   "source": [
    "# Zuschneiden der Datensätze --> gleiche Länge von X und y\n",
    "\n",
    "# Dateninput: 100%\n",
    "# Skalierte Daten\n",
    "# Trainingsdaten\n",
    "trainY_N100=trainY_N100.tolist()\n",
    "trainY_N100=trainY_N100[look_back-1:]\n",
    "trainY_N100=np.array(trainY_N100)\n",
    "print(trainX_N100.shape, trainY_N100.shape)\n",
    "# Testdaten\n",
    "testY_N100=testY_N100.tolist()\n",
    "testY_N100=testY_N100[look_back-1:]\n",
    "testY_N100=np.array(testY_N100)\n",
    "print(testX_N100.shape, testY_N100.shape)\n",
    "\n",
    "# Originaldaten\n",
    "# Trainingsdaten\n",
    "trainY_100=trainY_100.tolist()\n",
    "trainY_100=trainY_100[look_back-1:]\n",
    "trainY_100=np.array(trainY_100)\n",
    "print(trainX_100.shape, trainY_100.shape)\n",
    "# Testdaten\n",
    "testY_100=testY_100.tolist()\n",
    "testY_100=testY_100[look_back-1:]\n",
    "testY_100=np.array(testY_100)\n",
    "print(testX_100.shape, testY_100.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion: X,y trennen, Form von X anpassen, Modell zusammenbauen, Modell anpassen\n",
    "def fit_lstm(X, y , batch_size, blocks, epochen):\n",
    "    X=array(X)\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(blocks, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(1, activation='tanh'))\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=epochen, batch_size=1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "354/354 [==============================] - 3s 7ms/step - loss: -15.8840 - acc: 0.0000e+00\n",
      "Epoch 2/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 3/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 4/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 5/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 6/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 7/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 8/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 9/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 10/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00A: 1s - loss: -16\n",
      "Epoch 11/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 12/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 13/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 14/30\n",
      "354/354 [==============================] - 2s 5ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 15/30\n",
      "354/354 [==============================] - 2s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 16/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 17/30\n",
      "354/354 [==============================] - 2s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 18/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 19/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 20/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 21/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 22/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 23/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 24/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 25/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 26/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 27/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 28/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 29/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n",
      "Epoch 30/30\n",
      "354/354 [==============================] - 1s 4ms/step - loss: -16.1181 - acc: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "# Modell anpassen: 100%\n",
    "lstm_model_100 = fit_lstm(trainX_N100, trainY_N100, batch_size=1, blocks=15, epochen=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktion zur Vorhersage definieren\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "    X=array(X)\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    return yhat[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validierung des Modells anhand der Testdaten\n",
    "predictions_100 = list()\n",
    "for i in range(len(testY_N100)):\n",
    "    # schrittweise Vorhersage\n",
    "    X=testX_N100\n",
    "    yhat = forecast_lstm(lstm_model_100, 1, X.iloc[i,:])\n",
    "    # Vorhersage speichern\n",
    "    predictions_100.append(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Umwandeln der Werte in Trendzahlen [-1,0,1]\n",
    "def classification(dataset):\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i]>= 0.05:\n",
    "            dataset[i]=1\n",
    "        elif dataset[i]<= -0.05:\n",
    "            dataset[i]=-1\n",
    "        else:\n",
    "            dataset[i]=0\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "testPredict_100=classification(predictions_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7142857142857143"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(testY_N100, testPredict_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
